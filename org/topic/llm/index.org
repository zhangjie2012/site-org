#+SETUPFILE: ~/site/tpl/wiki-tpl.org
#+TITLE: LLM
#+DATE: 2025-02-05 21:27:15

#+begin_quote
个人理解，不保证正确
#+end_quote

* 目标

工程化学习 LLM 中上下游链路知识，了解涉及到的技术解决了什么问题（应用），而不是怎么解决的（原理）。

* 大模型三要素

- 算法
- 算力（GPU 卡）
- 数据（训练）

* 概念

- LLM，Large Language Model 大语言模型
- TTFT Time to First Token，首 token 到达的时间，问问题第一字蹦出来的时间
- TPOT Time Per-Output Token，Per token，之后每一个 token 蹦出来的时间
- batchsize：推理引擎能够同时并行处理的请求数
- 32k/64k/128k：模型能够接收输入的最大文本数
- 输入输出比：模型 Input 和 Output 的 token 比值
- prefill 和 decode，对应推理的两个阶段，分别负责输入（Prompt phase）和输出（Token generation phase）。prefill 对于算力要求较高（大模型计算并存储原始输入 token 的 KV Cache，并生成第一个输出 token）；decode 对于内存和带宽要求较高（因为 KV Cache，需要一级一级的判断）
  + prefill 预填充，在任务处理处理之前，提供一些初始数据和信息，以便模型更好地开始工作或更快地收敛到一个较好的结果。它主要是 *对输入进行初步处理和分析* ，为后续的操作奠定基础
  + decode 解码，通常是在模型经过一系列计算后，根据模型输出的概率分布等信息， *生成具体的输出结果* ，比如文本生成任务中的逐个生成后续的词元以形成完整的文本
- CUDA：CUDA（Compute Unified Device Architecture，统一计算架构）是由 NVIDIA 公司推出的一种并行计算平台和编程模型。它允许开发者使用 NVIDIA 的 GPU（图形处理单元）进行通用计算，
  即所谓的 GPGPU（General-Purpose computing on Graphics Processing Units，图形处理单元上的通用计算）。 *我的理解：GPU 原本的设计是处理图形的，基于 CUDA 可以用来做通用计算。*
- Distillation 蒸馏，模型压缩技术，主要将大型的、复杂的模型知识转移到一个较小、轻量的模型，以提高推理速度和计算效率，同时尽可能保持性能不下降太多。 *核心思想是让小模型（学生模型）学习大模型（教师模型）。*
- CoT（Chain of Thought）思维链，即整个大模型的推理过程（思考过程）
- 多模态模型（Multimodal Model）：不仅仅能够处理文本，还能理解、生成和融合多种数据模态（如图像、音频、视频）的模型

* 基础知识

** 训练（Training）和推理（Inference）

练是“教模型怎么做”的过程；推理是“让模型去做”的过程。 训练是学习，推理的应用。训练所需要的时间与参数量相关，小模型几小时、几天；中模型几天到几周；大模型数周和数月。

| 属性     | 训练（Training）                   | 推理（Inference）    |
|----------+------------------------------------+----------------------|
| 目的     | 学习输入输出映射关系，调整模型参数 | 利用模型生成预测结果 |
| 数据需求 | 输入 + 标签                        | 输入即可             |
| 计算需求 | 高（前向 + 反向传播）              | 低（仅前向传播）     |
| 时间消耗 | 长                                 | 短                   |
| 参数状态 | 动态调整                           | 固定                 |
| 硬件需求 | 高性能 GPU/TPU                     | GPU/CPU/边缘设备均可 |
| 典型场景 | 模型训练、微调                     | 实时推理、批量预测   |

** 分布式推理

将推理按照阶段拆分，然后把各个阶段的结果组合到一起，以提高效率。 *因为 prefill 和 decode 对与算力的要求不同，放在一起会出现拖累。* 分布式就是把 prefill 和 decode 分成两类服务。简单来说：

- 非分布式，是同一个服务来实现推理
- 分布式，分 prefill 和 decode 两个服务协作完成推理

为什么 LLM 做 benchmark 难？个人理解：对于传统的服务做压测是简单的，因为输出相对明确，输出也是相对明确的（输出是一次性给出的）。
反观 LLM 一次推理过程，返回的 Token 是连续给出的，推理耗时不光会受 Input Token, Output Token 长度的影响，而且也会受 Input/Output 比例影响。因此在实验室环境下，只能人为约定一个 Input/Output Ratio 来压测，但实际上用户的使用场景是千差万别的。

** GPU 的利用率

- Tensor Core Utility 是衡量 GPU 中 Tensor Cores 使用效率的指标，用于评估矩阵运算（如深度学习任务）是否充分利用了这些专用硬件单元的性能。
- SMActivity：Streaming Multiprocessor (SM) Activity，是衡量 GPU 利用率的重要指标。SM (Streaming Multiprocessor) 是 NVIDIA GPU 架构中的核心计算单元，每个 SM 包含若干个 CUDA 核心、纹理单元和其他硬件资源，用于并行处理任务。
- GPU Util：GPU 利用率是 GPU 的整体视图，除了了 SM（计算核心）之外，还有内存、带宽等。粒度比较粗糙。

| 指标             | Tensor Core Utility          | SMActivity                     | GPU Utilization                    |
|------------------+------------------------------+--------------------------------+------------------------------------|
| 关注层面         | Tensor Cores                 | GPU 的 SM 计算核心             | GPU 的整体利用率                   |
| 粒度             | 专注于矩阵计算硬件的使用效率 | 细粒度（专注计算核心）         | 粗粒度（计算 + 内存 + 数据传输等） |
| 是否计算内存操作 | -                            | -                              | 计算                               |
| 优化目标         |                              | 提高计算核心利用率             | 提高整体 GPU 资源利用率            |
| 适用场景         | 矩阵运算                     | 核心优化（如 CUDA 核函数调优） | GPU 是否被任务充分占用的整体评估   |

** MoE（专家混合）

Mixture of Experts 是一种深度学习模型架构。主要解决了 *大模型的计算效率和可扩展性问题* ，使得超大规模 Transformer 结构在计算资源受限的情况下仍然可以高效训练和推理。

- *计算更高效* 稀疏计算减少计算需求，提升训练和推理速度
- *大规模训练可行* 允许超大规模模型（万亿参数级）在有限计算资源下训练
- *任务泛化更强* 不同专家专注不同任务，提高多任务和多模态性能
- *负载均衡优化* 确保计算资源合理分配，避免专家过载或闲置

** AI Agent（智能体）

AI Agent（人工智能智能体） 是一种能够 *自主感知环境、做出决策并执行任务* 的 AI 系统。相比于传统 LLM（大语言模型）仅用于回答问题，AI Agent 具备更强的 *自主性和交互能力* ，可以持续与环境交互，完成复杂任务。

一个典型的 AI Agent 由以下部分组成：
1. 感知模块（Perception）
   + 输入：文本、图像、语音、环境数据。
   + 例如，语音助手通过麦克风接收用户语音指令。
2. 思维 / 规划（Reasoning / Planning）
   + 负责分析输入，并决定下一步行动。
   + 例如，AutoGPT 解析问题后，可能会搜索信息或编写代码。
3. 工具调用（Tool Use / Action Execution）
   + AI Agent 可以调用外部 API、数据库、搜索引擎、执行代码等。
   + 例如，Copilot 在编程时调用 IDE 进行代码补全。
4. 记忆（Memory）
   + 传统 LLM 只能处理当前对话，而 AI Agent 可以存储长期记忆，跨对话保持上下文。
   + 例如，ReAct 框架结合记忆与推理，允许 AI 进行多步推理。
5. 反馈循环（Feedback Loop）
   + AI Agent 执行任务后，会评估结果，调整策略，进行下一步行动。
   + 例如，AI 机器人在尝试登录失败后，会修改输入参数重试。

| 对比项     | 传统 LLM（如 GPT-4） | AI Agent（如 AutoGPT）     |
|------------+----------------------+----------------------------|
| 决策能力   | 被动回答问题         | 自主分析并执行任务         |
| 上下文记忆 | 仅限于当前对话       | 具备长期记忆               |
| 工具使用   | 依赖用户手动输入     | 可自动调用 API、运行代码   |
| 交互方式   | 单轮对话             | 持续交互、动态调整         |
| 应用范围   | 主要是文本处理       | 任务执行、数据分析、自动化 |

典型的应用场景：智能客服（独自处理用户咨询，提供售后支持）、编程助手（GitHub Copilot）、机器人和自动驾驶（结合摄像头、传感器、视觉等技术）

通俗的理解：AI Agent 让 AI 不只是“给建议”，而是“帮你把事情办了”。比如：智能家居助手，当你说“帮我调节房间温度”，普通 AI 会告诉你：“你可以用遥控器调节到 24 ℃”，但是 agent 直接“帮你把房间温度调节到舒适的温度，并且还能记忆你的习惯，下次自动调整”。

再通俗的理解：AI Agent 借助更多的技术（NLP、视觉、传感器等）丰富了 AI 的上下文信息，再解决一些自动化工具做一些事情，并且有了记忆功能，不断的优化自身。

* Awesome

- [[https://github.com/Hannibal046/Awesome-LLM][Awesome-LLM]]
- [[https://github.com/Shubhamsaboo/awesome-llm-apps][awesome-llm-apps]]
- [[https://www.wangrs.site/awesome-LLM-resourses/][awesome-LLM-resourses]] 学习资料
